---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Roman Mutel, Liubomyr Mokrytskyi, Dmytro Mykytenko*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

**Sentiment: all the text messages contained in this data set are
labeled with three sentiments: positive, neutral or negative. The task
is to classify some text message as the one of positive mood, negative
or neutral.**

The data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

### Data pre-processing

```{r}
list.files(getwd())
list.files("3-sentiment")
```

```{r}
test_path <- "3-sentiment/test.csv"
train_path <- "3-sentiment/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split=c('\n'))
splitted_stop_words <- splitted_stop_words[[1]]
stop_words = gsub("[\r]", "", splitted_stop_words)
```

```{r}
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% stop_words & nchar(splitted) > 2)
count(tidy_text, splitted,sort=TRUE)[(1:10),]
```

### Data Visualization

Below is bar plot for words, which are longer than 3 symbols and
appeared more than 10 times in **negative** sentiment statements:

```{r}
negative <- tidy_text %>% filter(sentiment=="negative") %>% count(splitted,sort=TRUE) %>% filter(n > 10) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
negative <- mutate(negative, splitted = reorder(splitted, 1/n))
# negative
ggplot(negative, aes(splitted, n)) + geom_col()
```

...and a similar plot for 3+ symbols long words which appeared in
**positive** sentiment statements more than 100 times (there is more
positive statements, than negative, so the threshold of appearances is
larger than for negative words)

```{r}
positive <- tidy_text %>% filter(sentiment=="positive") %>% count(splitted,sort=TRUE) %>% filter(n > 100) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
positive <- mutate(positive, splitted = reorder(splitted, 1/n))
# positive
ggplot(positive, aes(splitted, n)) +geom_col()
```

## Classifier implementation (Authors Dataset Version)

```{r}
naiveBayes <- setRefClass("naiveBayes",
       fields = c("fdict", "total_prob_lovecraft", "total_prob_shelley", "total_prob_poe"),
       methods = list(
                    fit = function(X, y)
                    {
                         df <- data.frame(text=X,author=y)
                         stop_words <- strsplit(read_file("stop_words.txt"), split='\n')[[1]]
                         stop_words = gsub("[\r]", "", stop_words)
                         tidy_text <- unnest_tokens(df, 'token', 'text', token="words") %>% filter(!token %in% stop_words)
                         #print(tidy_text[(1:20),])
                         prob_authors <- count(tidy_text, author)
                         total_prob_poe <<- prob_authors[1,"n"]/sum(prob_authors$n)
                         total_prob_shelley <<- prob_authors[3,"n"]/sum(prob_authors$n)
                         total_prob_lovecraft <<- prob_authors[2,"n"]/sum(prob_authors$n)

                         fdict <<- data.frame(
                           word = count(tidy_text, token, sort=TRUE)$token
                           #total = count(tidy_text, token, sort=TRUE)$n
                         )
                         lovecraft <- tidy_text %>% filter(author=="HP Lovecraft")  %>% count(token, sort=TRUE)
                         total_lovecraft = nrow(lovecraft)
                         shelley <- tidy_text %>% filter(author=="Mary Wollstonecraft Shelley ") %>% count(token, sort=TRUE)
                         total_shelley = nrow(shelley)
                         poe <- tidy_text %>% filter(author=="Edgar Alan Poe")  %>% count(token, sort=TRUE)
                         total_poe = nrow(poe)
                         total_all = n_distinct(tidy_text$token)

                         fdict$prob_lovecraft <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_shelley <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_poe <<- seq(0,0, length=length(fdict$word))

                         for(i in 1:nrow(fdict)){
                           i_lovecraft = lovecraft[lovecraft$token==fdict[i,"word"],]$n
                           i_shelley = shelley[shelley$token==fdict[i,"word"],]$n
                           i_poe = poe[poe$token==fdict[i,"word"],]$n

                           if (length(i_lovecraft) == 0){
                              i_lovecraft <- integer(1)
                              i_lovecraft = 0
                           }
                           if (length(i_shelley) == 0){
                              i_shelley <- integer(1)
                              i_shelley = 0
                           }
                           if (length(i_poe) == 0){
                              i_poe <- integer(1)
                              i_poe = 0
                           }
                           
                           fdict[i,"prob_lovecraft"] <<- (i_lovecraft + 1) / (total_lovecraft + total_all)
                           fdict[i,"prob_shelley"] <<- (i_shelley + 1) / (total_shelley + total_all)
                           fdict[i,"prob_poe"] <<- (i_poe + 1) / (total_poe + total_all)
                         }
                    },
                    predict = function(message)
                    {
                         tokens = data.frame(
                           word = unlist(strsplit(message, split=" "))
                         )
                         tokens <-  filter(tokens, !word %in% stop_words & word %in% fdict$word)
                         filtered_fdict <- filter(fdict, word %in% tokens$word)
                         prob_lovecraft <- prod(filtered_fdict$prob_lovecraft) * total_prob_lovecraft
                         prob_shelley <- prod(filtered_fdict$prob_shelley) * total_prob_shelley
                         prob_poe <- prod(filtered_fdict$prob_poe) * total_prob_poe
                         # print("Probability for message:")
                         # print(message)
                         # print("P(Lovecraft|message):")
                         # print(prob_lovecraft)
                         # print("P(Shelley|message):")
                         # print(prob_shelley)
                         # print("P(Poe|message):")
                         # print(prob_poe)
                         if (prob_lovecraft > prob_shelley && prob_lovecraft > prob_poe){
                           return("HP Lovecraft")
                         }
                         else if(prob_poe > prob_shelley){
                           return("Edgar Alan Poe")
                         }
                         else{
                           return("Mary Wollstonecraft Shelley ")
                         }

                    },
                    score = function(X_test, y_test)
                    {
                         df <- data.frame(X_test,y_test)
                         df$prediction = seq(0,0,length=nrow(df))
                         counter = 0
                         for(i in (1:nrow(df))){
                           df[i,"prediction"] = .self$predict(df[i,"X_test"])
                           if(df[i,"prediction"] == df[i,"y_test"]){
                             counter = counter + 1
                           }
                         }
                         print(df[(1:20),c("X_test", "prediction", "y_test")])
                         return(counter/nrow(df))
                    }
))
test_path <- "0-authors/test.csv"
train_path <- "0-authors/train.csv"
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)

model = naiveBayes()
model$fit(train$text, train$author)
model$fdict[(1:20),]
model$predict("My mother's tender caresses and my father's smile of benevolent pleasure while regarding me are my first recollections.")
print("Overall score:")
model$score(test$text, test$author)
```

## Summary

In this lab task, we used **Bayes' rule** as the main mathematical model
of probability search and **bag of words** technique (ignore the order
of words in a sentence and count only the frequency) to improve the
accuracy of the similarity search algorithm. To classify the sentences
we calculated the **conditional probability** of each word given the
text type, and then calculated the **product** of these probabilities
for all known words in given sentence (as we assumed the words are
independent, multiplying their probabilities will denote the probability
of the whole sentence). Then we compared the probabilities and chose the
largest one.

In the main task, we developed 3 methods of the `naiveBayes` class:

-   `fit` - determines the probability of each word belonging to a
    specific "tag", creates frequency dictionary
-   `predict` - using the formula described above, finds the probability
    that the sentence is under one of the "tags".
-   `score` -- tests the code and determines the accuracy of the
    algorithm.

Naive Bayes is a good classifying method even despite it "naive" part
where we make a strong **assumption**, that word order does not matter
and the probability of each word in sentence is independent, which lets
us use the **bag of words** technique. Its main pros are (usually)
relatively good **accuracy** and high **speed**. Its con is that in
specific cases word order and word combinations may be **more useful**
than single word bag of words approach. Also, no model works well if it
was trained on invalid data which does not illustrate real conditions.

In our case, the accuracy is **low**. It would be larger if we were
given **more diverse data**: in train.csv we have only **119** entries
of **negative** sentiment sentences among **4200** **total** examples,
which causes conditional probability of words not fair: in the smaller
datasets such as our negative sentences dataset there is more unique
words which means it is harder to find class-specific words which makes
classifying harder. **Larger portions** of data to train our model (for
example whole article instead of just the headline) would also improve
accuracy of the model.
