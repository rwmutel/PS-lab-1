---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Roman Mutel, Liubomyr Mokrytskyi, Dmytro Mykytenko*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 datasets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   0 - authors This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   1 - discrimination This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   2 - fake news This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment All the text messages contained in this data set are
    labeled with three sentiments: positive, neutral or negative. The
    task is to classify some text message as the one of positive mood,
    negative or neutral.**

-   4 - spam This last data set contains SMS messages classified as spam
    or non-spam (ham in the data set). The task is to determine whether
    a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("3-sentiment")
```

```{r}
test_path <- "3-sentiment/test.csv"
train_path <- "3-sentiment/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split=c('\n'))
splitted_stop_words <- splitted_stop_words[[1]]
stop_words = gsub("[\r]", "", splitted_stop_words)
```

```{r}
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% stop_words & nchar(splitted) > 2)
tidy_text %>% count(splitted,sort=TRUE)
```

### Data Visualization

Below is bar plot for words, which are longer than 3 symbols and
appeared more than 10 times in **negative** sentiment statements:

```{r}
negative <- tidy_text %>% filter(sentiment=="negative") %>% count(splitted,sort=TRUE) %>% filter(n > 10) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
negative <- mutate(negative, splitted = reorder(splitted, 1/n))
# negative
ggplot(negative, aes(splitted, n)) + geom_col()
```

...and a similar plot for 3+ symbols long words which appeared in
**positive** sentiment statements more than 100 times (there is more
positive statements, than negative, so the threshold of appearances is
larger than for negative words)

```{r}
positive <- tidy_text %>% filter(sentiment=="positive") %>% count(splitted,sort=TRUE) %>% filter(n > 100) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
positive <- mutate(positive, splitted = reorder(splitted, 1/n))
# positive
ggplot(positive, aes(splitted, n)) +geom_col()
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
       fields = c("fdict", "total_prob_pos", "total_prob_neu", "total_prob_neg"),
       methods = list(
                    fit = function(X, y)
                    {
                         df <- data.frame(text=X,sentiment=y)
                         stop_words <- strsplit(read_file("stop_words.txt"), split='\n')[[1]]
                         stop_words = gsub("[\r]", "", stop_words)
                         tidy_text <- unnest_tokens(df, 'token', 'text', token="words") %>% filter(!token %in% stop_words)
                         prob_sentiments <- count(tidy_text, sentiment)
                         total_prob_neg <<- prob_sentiments[1,"n"]/sum(prob_sentiments$n)
                         total_prob_neu <<- prob_sentiments[2,"n"]/sum(prob_sentiments$n)
                         total_prob_pos <<- prob_sentiments[3,"n"]/sum(prob_sentiments$n)

                         fdict <<- data.frame(
                           word = count(tidy_text, token, sort=TRUE)$token
                           #total = count(tidy_text, token, sort=TRUE)$n
                         )
                         positive <- tidy_text %>% filter(sentiment=="positive")  %>% count(token, sort=TRUE)
                         total_pos = nrow(positive)
                         neutral <- tidy_text %>% filter(sentiment=="neutral") %>% count(token, sort=TRUE)
                         total_neu = nrow(neutral)
                         negative <- tidy_text %>% filter(sentiment=="negative")  %>% count(token, sort=TRUE)
                         total_neg = nrow(negative)
                         total_all = n_distinct(tidy_text$token)

                         fdict$prob_pos <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_neu <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_neg <<- seq(0,0, length=length(fdict$word))

                         for(i in 1:nrow(fdict)){
                           i_pos = positive[positive$token==fdict[i,"word"],]$n
                           i_neu = neutral[neutral$token==fdict[i,"word"],]$n
                           i_neg = negative[negative$token==fdict[i,"word"],]$n

                           if (length(i_pos) == 0){
                              i_pos <- integer(1)
                              i_pos = 0
                           }
                           if (length(i_neu) == 0){
                              i_neu <- integer(1)
                              i_neu = 0
                           }
                           if (length(i_neg) == 0){
                              i_neg <- integer(1)
                              i_neg = 0
                           }
                           
                           fdict[i,"prob_pos"] <<- (i_pos + 1) / (total_pos + total_all)
                           fdict[i,"prob_neu"] <<- (i_neu + 1) / (total_neu + total_all)
                           fdict[i,"prob_neg"] <<- (i_neg + 1) / (total_neg + total_all)
                         }
                         fdict$prob_pos <<- fdict$prob_pos / total_prob_pos
                         fdict$prob_neu <<- fdict$prob_neu / total_prob_neu
                         fdict$prob_neg <<- fdict$prob_neg / total_prob_neg
                    },
                    predict = function(message)
                    {
                         tokens <- read.table(text=message,col.names=c("word")) %>% filter(!word %in% stop_words & word %in% fdict$word)
                         print(tokens)
                         filtered_fdict <- filter(fdict, word %in% tokens$word)
                         print(filtered_fdict)
                         prob_pos <- prod(filtered_fdict$prob_pos) * total_prob_pos
                         prob_neu <- prod(filtered_fdict$prob_neu) * total_prob_neu
                         prob_neg <- prod(filtered_fdict$prob_neg) * total_prob_neg
                         print(prob_pos)
                         print(prob_neu)
                         print(prob_neg)
                         if (prob_pos > prob_neu & prob_pos > prob_neg){
                           return("positive")
                         }
                         else if(prob_pos > prob_neg){
                           return("neutral")
                         }
                         else{
                           return("negative")
                         }

                    },
                    score = function(X_test, y_test)
                    {
                         # TODO
                    }
))
test_path <- "3-sentiment/test.csv"
train_path <- "3-sentiment/train.csv"
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)

model = naiveBayes()
model$fit(train$text, train$sentiment)
model$fdict
model$predict("Cramo slipped to a pretax loss of EUR 6.7 million from a pretax profit of EUR 58.9 million .")
model$predict("According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .")
```

"

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.
-   Visualize them.
-   Show failure cases.

## Conclusions

Summarize your work by explaining in a few sentences the points listed
below.

-   Describe the method implemented in general. Show what are
    mathematical foundations you are basing your solution on.
-   List pros and cons of the method. This should include the
    limitations of your method, all the assumption you make about the
    nature of your data etc.
