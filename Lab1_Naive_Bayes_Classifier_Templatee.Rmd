---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

### *Roman Mutel, Liubomyr Mokrytskyi, Dmytro Mykytenko*

## Introduction

During the past three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the *Bayes
theorem*.

One of its applications is **Naive Bayes classifier**, which is a
probabilistic classifier whose aim is to determine which class some
observation probably belongs to by using the Bayes formula:
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong independence assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given
observation. Thus, $\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now
can be calculated as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated from the data as
respective relative frequencies;\
see [this
site](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
for more detailed explanations.

## Data description

There are 5 datasets uploaded on the cms.

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   0 - authors This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   1 - discrimination This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   2 - fake news This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment All the text messages contained in this data set are
    labeled with three sentiments: positive, neutral or negative. The
    task is to classify some text message as the one of positive mood,
    negative or neutral.**

-   4 - spam This last data set contains SMS messages classified as spam
    or non-spam (ham in the data set). The task is to determine whether
    a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one you will need find the probabilities distributions for each of
the features, while the second one is needed for checking how well your
classifier works.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

## Instructions

-   The first step is data pre-processing, which includes removing
    punctuation marks and stop words

-   represent each message as a bag-of-words

-   using the training set, calculate all the conditional probabilities
    in formula (1)

-   use those to predict classes for messages in the test set

-   evaluate effectiveness of the classifier by calculating the
    corresponding metrics

-   shortly summarize your work

-   do not forget to submit both the (compiled) Rmd source file and the
    .html output

### Data pre-processing

-   Read the *.csv* data files.
-   Ð¡lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())
list.files("3-sentiment")
```

```{r}
test_path <- "3-sentiment/test.csv"
train_path <- "3-sentiment/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split=c('\n'))
splitted_stop_words <- splitted_stop_words[[1]]
stop_words = gsub("[\r]", "", splitted_stop_words)
```

```{r}
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)
```

```{r}
# note the power functional features of R bring us! 
tidy_text <- unnest_tokens(train, 'splitted', 'text', token="words") %>%
             filter(!splitted %in% stop_words & nchar(splitted) > 2)
tidy_text %>% count(splitted,sort=TRUE)
```

### Data Visualization

Below is bar plot for words, which are longer than 3 symbols and
appeared more than 10 times in **negative** sentiment statements:

```{r}
negative <- tidy_text %>% filter(sentiment=="negative") %>% count(splitted,sort=TRUE) %>% filter(n > 10) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
negative <- mutate(negative, splitted = reorder(splitted, 1/n))
# negative
ggplot(negative, aes(splitted, n)) + geom_col()
```

...and a similar plot for 3+ symbols long words which appeared in
**positive** sentiment statements more than 100 times (there is more
positive statements, than negative, so the threshold of appearances is
larger than for negative words)

```{r}
positive <- tidy_text %>% filter(sentiment=="positive") %>% count(splitted,sort=TRUE) %>% filter(n > 100) %>% filter(nchar(splitted) > 3)
# sorting our dataframe
positive <- mutate(positive, splitted = reorder(splitted, 1/n))
# positive
ggplot(positive, aes(splitted, n)) +geom_col()
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass("naiveBayes",
       fields = c("fdict", "total_prob_pos", "total_prob_neu", "total_prob_neg"),
       methods = list(
                    fit = function(X, y)
                    {
                         df <- data.frame(text=X,sentiment=y)
                         stop_words <- strsplit(read_file("stop_words.txt"), split='\n')[[1]]
                         stop_words = gsub("[\r]", "", stop_words)
                         tidy_text <- unnest_tokens(df, 'token', 'text', token="words") %>% filter(!token %in% stop_words)
                         prob_sentiments <- count(tidy_text, sentiment)
                         total_prob_neg <<- prob_sentiments[1,"n"]/sum(prob_sentiments$n)
                         total_prob_neu <<- prob_sentiments[2,"n"]/sum(prob_sentiments$n)
                         total_prob_pos <<- prob_sentiments[3,"n"]/sum(prob_sentiments$n)

                         fdict <<- data.frame(
                           word = count(tidy_text, token, sort=TRUE)$token
                           #total = count(tidy_text, token, sort=TRUE)$n
                         )
                         positive <- tidy_text %>% filter(sentiment=="positive")  %>% count(token, sort=TRUE)
                         total_pos = nrow(positive)
                         neutral <- tidy_text %>% filter(sentiment=="neutral") %>% count(token, sort=TRUE)
                         total_neu = nrow(neutral)
                         negative <- tidy_text %>% filter(sentiment=="negative")  %>% count(token, sort=TRUE)
                         total_neg = nrow(negative)
                         total_all = n_distinct(tidy_text$token)

                         fdict$prob_pos <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_neu <<- seq(0,0, length=length(fdict$word))
                         fdict$prob_neg <<- seq(0,0, length=length(fdict$word))

                         for(i in 1:nrow(fdict)){
                           i_pos = positive[positive$token==fdict[i,"word"],]$n
                           i_neu = neutral[neutral$token==fdict[i,"word"],]$n
                           i_neg = negative[negative$token==fdict[i,"word"],]$n

                           if (length(i_pos) == 0){
                              i_pos <- integer(1)
                              i_pos = 0
                           }
                           if (length(i_neu) == 0){
                              i_neu <- integer(1)
                              i_neu = 0
                           }
                           if (length(i_neg) == 0){
                              i_neg <- integer(1)
                              i_neg = 0
                           }
                           
                           fdict[i,"prob_pos"] <<- (i_pos + 1) / (total_pos + total_all)
                           fdict[i,"prob_neu"] <<- (i_neu + 1) / (total_neu + total_all)
                           fdict[i,"prob_neg"] <<- (i_neg + 1) / (total_neg + total_all)
                         }
                    },
                    predict = function(message)
                    {
                         tokens = data.frame(
                           word = unlist(strsplit(message, split=" "))
                         )
                         tokens <-  filter(tokens, !word %in% stop_words & word %in% fdict$word)
                         filtered_fdict <- filter(fdict, word %in% tokens$word)
                         prob_pos <- prod(filtered_fdict$prob_pos) * total_prob_pos
                         prob_neu <- prod(filtered_fdict$prob_neu) * total_prob_neu
                         prob_neg <- prod(filtered_fdict$prob_neg) * total_prob_neg
                         #print(prob_pos)
                         #print(prob_neu)
                         #print(prob_neg)
                         if (prob_pos > prob_neu & prob_pos > prob_neg){
                           return("positive")
                         }
                         else if(prob_pos > prob_neg){
                           return("neutral")
                         }
                         else{
                           return("negative")
                         }

                    },
                    score = function(X_test, y_test)
                    {
                         df <- data.frame(X_test,y_test)
                         df$prediction = seq(0,0,length=nrow(df))
                         counter = 0
                         for(i in (1:nrow(df))){
                           df[i,"prediction"] = .self$predict(df[i,"X_test"])
                           if(df[i,"prediction"] == df[i,"y_test"]){
                             counter = counter + 1
                           }
                         }
                         #print(df)
                         return(counter/nrow(df))
                    }
))
test_path <- "3-sentiment/test.csv"
train_path <- "3-sentiment/train.csv"
train <- read.csv(file = train_path, stringsAsFactors = FALSE)
test <- read.csv(file = test_path, stringsAsFactors = FALSE)

model = naiveBayes()
model$fit(train$text, train$sentiment)
model$fdict
model$predict("Cramo slipped to a pretax loss of EUR 6.7 million from a pretax profit of EUR 58.9 million .")
#model$predict("According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .")
model$score(test$text, test$sentiment)
```

## Summary

In this lab task, we used **Bayes' rule** as the main mathematical model
of probability search and **bag of words** technique (ignore the order
of words in a sentence and count only the frequency) to improve the
accuracy of the similarity search algorithm. To classify the sentences
we calculated the **conditional probability** of each word given the
text type, and then calculated the **product** of these probabilities
for all known words in given sentence (as we assumed the words are
independent, multiplying their probabilities will denote the probability
of the whole sentence). Then we compared the probabilities and chose the
largest one.

In the main task, we developed 3 methods of the `naiveBayes` class:

-   `fit` - determines the probability of each word belonging to a
    specific "tag", creates frequency dictionary
-   `predict` - using the formula described above, finds the probability
    that the sentence is under one of the "tags".
-   `score` -- tests the code and determines the accuracy of the
    algorithm.

Naive Bayes is a good classifying method even despite it "naive" part
where we make a strong **assumption**, that word order does not matter
and the probability of each word in sentence is independent, which lets
us use the **bag of words** technique. Its main pros are (usually)
relatively good **accuracy** and high **speed**. Its con is that in
specific cases word order and word combinations may be **more useful**
than single word bag of words approach. Also, no model works well if it
was trained on invalid data which does not illustrate real conditions.

In our case, the accuracy is **low**. It would be larger if we were
given **more diverse data**: in train.csv we have only **119** entries
of **negative** sentiment sentences among **4200** **total** examples,
which causes conditional probability of words not fair: in the smaller
datasets such as our negative sentences dataset there is more unique
words which means it is harder to find class-specific words which makes
classifying harder. **Larger portions** of data to train our model (for
example whole article instead of just the headline) would also improve
accuracy of the model.
